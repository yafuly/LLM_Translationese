{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7d4aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import stanza\n",
    "import re\n",
    "\n",
    "# Initialize stanza pipelines for tokenization\n",
    "stanza.download('en')\n",
    "stanza.download('zh')\n",
    "nlp_en = stanza.Pipeline('en', processors='tokenize,pos')\n",
    "nlp_zh = stanza.Pipeline('zh', processors='tokenize,pos')\n",
    "\n",
    "def tokenize_text(text, is_zh=False):\n",
    "    if is_zh:\n",
    "        doc = nlp_zh(text)\n",
    "    else:\n",
    "        doc = nlp_en(text)\n",
    "    return [word.text for sent in doc.sentences for word in sent.words]\n",
    "\n",
    "def calculate_length_variety(source_sentence, target_sentence, source_is_zh=False, target_is_zh=False):\n",
    "    len_source = len(tokenize_text(source_sentence, source_is_zh))\n",
    "    len_target = len(tokenize_text(target_sentence, target_is_zh))\n",
    "    length_variety = abs(len_source - len_target) / len_source if len_source > 0 else 0\n",
    "    return length_variety\n",
    "\n",
    "def calculate_global_lexical_density(corpus, is_zh=False):\n",
    "    all_tokens = []\n",
    "    content_words = []\n",
    "    for text in corpus:\n",
    "        if is_zh:\n",
    "            doc = nlp_zh(text)\n",
    "        else:\n",
    "            doc = nlp_en(text)\n",
    "        all_tokens.extend([word.text for sent in doc.sentences for word in sent.words])\n",
    "        content_words.extend([word.text for sent in doc.sentences for word in sent.words \n",
    "                              if word.upos in ['NOUN', 'VERB', 'ADJ', 'ADV']])\n",
    "    lexical_density = len(content_words) / len(all_tokens) if len(all_tokens) > 0 else 0\n",
    "    return lexical_density\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572f21d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the path to the data\n",
    "path = \"./llama_results\"\n",
    "\n",
    "# Initialize results dictionary\n",
    "results = {}\n",
    "\n",
    "# Process files in the given path\n",
    "for file_path in glob.glob(f\"{path}/*.json\"):\n",
    "    if \"_0\" in file_path:\n",
    "        continue\n",
    "\n",
    "    print(file_path.split(\"/\")[-1])\n",
    "    is_zh = \"deen\" not in file_path\n",
    "\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Extract source and target texts\n",
    "    sources = [_extract_text(item[\"prompt\"], is_zh=is_zh) for item in data]\n",
    "    translations = [item[\"predict\"] for item in data]\n",
    "\n",
    "    # Calculate metrics\n",
    "    lex_density = calculate_global_lexical_density(translations, is_zh=is_zh)\n",
    "    len_variety = [\n",
    "        calculate_length_variety(s, t, source_is_zh=False, target_is_zh=is_zh)\n",
    "        for s, t in tqdm(zip(sources, translations))\n",
    "    ]\n",
    "\n",
    "    # Store results\n",
    "    results[file_path.split(\"/\")[-1]] = {\n",
    "        \"lex_density\": lex_density,\n",
    "        \"len_variety\": np.mean(len_variety),\n",
    "    }\n",
    "\n",
    "    print(f\"Lexical Density: {lex_density}, Length Variety: {np.mean(len_variety)}\")\n",
    "\n",
    "# Output results\n",
    "print(results)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
