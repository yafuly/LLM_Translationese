{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e7d4aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json: 392kB [00:00, 5.89MB/s]                    \n",
      "2025-01-26 11:02:22 INFO: Downloaded file to /Users/haroldl/stanza_resources/resources.json\n",
      "2025-01-26 11:02:22 INFO: Downloading default packages for language: en (English) ...\n",
      "2025-01-26 11:02:24 INFO: File exists: /Users/haroldl/stanza_resources/en/default.zip\n",
      "2025-01-26 11:02:26 INFO: Finished downloading models and saved to /Users/haroldl/stanza_resources\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json: 392kB [00:00, 6.44MB/s]                    \n",
      "2025-01-26 11:02:26 INFO: Downloaded file to /Users/haroldl/stanza_resources/resources.json\n",
      "2025-01-26 11:02:26 INFO: \"zh\" is an alias for \"zh-hans\"\n",
      "2025-01-26 11:02:26 INFO: Downloading default packages for language: zh-hans (Simplified_Chinese) ...\n",
      "2025-01-26 11:02:28 INFO: File exists: /Users/haroldl/stanza_resources/zh-hans/default.zip\n",
      "2025-01-26 11:02:32 INFO: Finished downloading models and saved to /Users/haroldl/stanza_resources\n",
      "2025-01-26 11:02:32 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json: 392kB [00:00, 11.7MB/s]                    \n",
      "2025-01-26 11:02:32 INFO: Downloaded file to /Users/haroldl/stanza_resources/resources.json\n",
      "2025-01-26 11:02:32 WARNING: Language en package default expects mwt, which has been added\n",
      "2025-01-26 11:02:33 INFO: Loading these models for language: en (English):\n",
      "===============================\n",
      "| Processor | Package         |\n",
      "-------------------------------\n",
      "| tokenize  | combined        |\n",
      "| mwt       | combined        |\n",
      "| pos       | combined_charlm |\n",
      "===============================\n",
      "\n",
      "2025-01-26 11:02:33 INFO: Using device: cpu\n",
      "2025-01-26 11:02:33 INFO: Loading: tokenize\n",
      "2025-01-26 11:02:33 INFO: Loading: mwt\n",
      "2025-01-26 11:02:33 INFO: Loading: pos\n",
      "2025-01-26 11:02:33 INFO: Done loading processors!\n",
      "2025-01-26 11:02:33 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json: 392kB [00:00, 7.19MB/s]                    \n",
      "2025-01-26 11:02:33 INFO: Downloaded file to /Users/haroldl/stanza_resources/resources.json\n",
      "2025-01-26 11:02:33 INFO: \"zh\" is an alias for \"zh-hans\"\n",
      "2025-01-26 11:02:34 INFO: Loading these models for language: zh-hans (Simplified_Chinese):\n",
      "==============================\n",
      "| Processor | Package        |\n",
      "------------------------------\n",
      "| tokenize  | gsdsimp        |\n",
      "| pos       | gsdsimp_charlm |\n",
      "==============================\n",
      "\n",
      "2025-01-26 11:02:34 INFO: Using device: cpu\n",
      "2025-01-26 11:02:34 INFO: Loading: tokenize\n",
      "2025-01-26 11:02:34 INFO: Loading: pos\n",
      "2025-01-26 11:02:35 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import glob\n",
    "import json\n",
    "import nltk\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import stanza\n",
    "import re\n",
    "\n",
    "# Initialize stanza pipelines for tokenization\n",
    "stanza.download('en')\n",
    "stanza.download('zh')\n",
    "nlp_en = stanza.Pipeline('en', processors='tokenize,pos')\n",
    "nlp_zh = stanza.Pipeline('zh', processors='tokenize,pos')\n",
    "\n",
    "\n",
    "\n",
    "def tokenize_text(text, is_zh=False):\n",
    "    def tokenize_chinese(text):\n",
    "        # char-level tokenize\n",
    "        # Regular expression to match Chinese characters, English words, and numbers\n",
    "        pattern = re.compile(r'[\\u4e00-\\u9fff]|[a-zA-Z]+|\\d+')\n",
    "        # Find all matches of Chinese characters, English words, and numbers\n",
    "        tokens = pattern.findall(text)\n",
    "        return tokens\n",
    "    if is_zh:\n",
    "        return tokenize_chinese(text)\n",
    "    else:\n",
    "\n",
    "        return nltk.word_tokenize(text)\n",
    "    \n",
    "\n",
    "\n",
    "def calculate_length_variety(source_sentence, target_sentence, source_is_zh=False, target_is_zh=False):\n",
    "    len_source = len(tokenize_text(source_sentence, source_is_zh))\n",
    "    len_target = len(tokenize_text(target_sentence, target_is_zh))\n",
    "    length_variety = abs(len_source - len_target) / len_source if len_source > 0 else 0\n",
    "    return length_variety\n",
    "\n",
    "def calculate_global_lexical_density(corpus, is_zh=False):\n",
    "    all_tokens = []\n",
    "    content_words = []\n",
    "    for text in corpus:\n",
    "        if is_zh:\n",
    "            doc = nlp_zh(text)\n",
    "        else:\n",
    "            doc = nlp_en(text)\n",
    "        all_tokens.extend([word.text for sent in doc.sentences for word in sent.words])\n",
    "        content_words.extend([word.text for sent in doc.sentences for word in sent.words \n",
    "                              if word.upos in ['NOUN', 'VERB', 'ADJ', 'ADV']])\n",
    "    lexical_density = len(content_words) / len(all_tokens) if len(all_tokens) > 0 else 0\n",
    "    return lexical_density\n",
    "\n",
    "def _extract_text(src, is_zh=False):\n",
    "    sep = \"Chinese\" if is_zh else \"English\"\n",
    "    src = src.split(\"assistant\")[0].split(f\"{sep}:\\n\")[1]\n",
    "\n",
    "    return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "572f21d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merge_pol_checkpoint-3500_predict_wmt_generated_predictions.jsonl.ppl.std.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2037it [00:00, 19005.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexical Density: 0, Length Variety: 0.4660532085327173\n",
      "merge_trans_checkpoint-3500_predict_deen_wmt_generated_predictions.jsonl.ppl.std.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1984it [00:00, 11384.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexical Density: 0, Length Variety: 0.15308827343544776\n",
      "merge_pol_checkpoint-3500_predict_generated_predictions.jsonl.ppl.std.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:00, 2212.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexical Density: 0, Length Variety: 0.7173388985989072\n",
      "merge_trans_checkpoint-3500_predict_generated_predictions.jsonl.ppl.std.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:00, 2305.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexical Density: 0, Length Variety: 0.648249274780602\n",
      "merge_pol_checkpoint-3500_predict_deen_wmt_generated_predictions.jsonl.ppl.std.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1984it [00:00, 10872.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexical Density: 0, Length Variety: 0.16479223973499815\n",
      "merge_raw_checkpoint-3500_predict_deen_generated_predictions.jsonl.ppl.std.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 1389.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexical Density: 0, Length Variety: 0.07860129601380215\n",
      "merge_raw_checkpoint-3500_predict_deen_wmt_generated_predictions.jsonl.ppl.std.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1984it [00:00, 11592.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexical Density: 0, Length Variety: 0.15028778091236628\n",
      "merge_raw_checkpoint-3500_predict_generated_predictions.jsonl.ppl.std.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:00, 2435.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexical Density: 0, Length Variety: 0.6389755572310938\n",
      "merge_raw_checkpoint-3500_predict_wmt_generated_predictions.jsonl.ppl.std.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2037it [00:00, 22395.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexical Density: 0, Length Variety: 0.3767721269648037\n",
      "merge_trans_checkpoint-3500_predict_wmt_generated_predictions.jsonl.ppl.std.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2037it [00:00, 21673.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexical Density: 0, Length Variety: 0.4059985250566517\n",
      "merge_pol_checkpoint-3500_predict_deen_generated_predictions.jsonl.ppl.std.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 1433.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexical Density: 0, Length Variety: 0.08022164224523846\n",
      "merge_trans_checkpoint-3500_predict_deen_generated_predictions.jsonl.ppl.std.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 1423.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexical Density: 0, Length Variety: 0.07748669704625583\n",
      "{'merge_pol_checkpoint-3500_predict_wmt_generated_predictions.jsonl.ppl.std.json': {'len_variety': 0.4660532085327173}, 'merge_trans_checkpoint-3500_predict_deen_wmt_generated_predictions.jsonl.ppl.std.json': {'len_variety': 0.15308827343544776}, 'merge_pol_checkpoint-3500_predict_generated_predictions.jsonl.ppl.std.json': {'len_variety': 0.7173388985989072}, 'merge_trans_checkpoint-3500_predict_generated_predictions.jsonl.ppl.std.json': {'len_variety': 0.648249274780602}, 'merge_pol_checkpoint-3500_predict_deen_wmt_generated_predictions.jsonl.ppl.std.json': {'len_variety': 0.16479223973499815}, 'merge_raw_checkpoint-3500_predict_deen_generated_predictions.jsonl.ppl.std.json': {'len_variety': 0.07860129601380215}, 'merge_raw_checkpoint-3500_predict_deen_wmt_generated_predictions.jsonl.ppl.std.json': {'len_variety': 0.15028778091236628}, 'merge_raw_checkpoint-3500_predict_generated_predictions.jsonl.ppl.std.json': {'len_variety': 0.6389755572310938}, 'merge_raw_checkpoint-3500_predict_wmt_generated_predictions.jsonl.ppl.std.json': {'len_variety': 0.3767721269648037}, 'merge_trans_checkpoint-3500_predict_wmt_generated_predictions.jsonl.ppl.std.json': {'len_variety': 0.4059985250566517}, 'merge_pol_checkpoint-3500_predict_deen_generated_predictions.jsonl.ppl.std.json': {'len_variety': 0.08022164224523846}, 'merge_trans_checkpoint-3500_predict_deen_generated_predictions.jsonl.ppl.std.json': {'len_variety': 0.07748669704625583}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the path to the data\n",
    "path = \"./llama_results\"\n",
    "\n",
    "# Initialize results dictionary\n",
    "results = {}\n",
    "\n",
    "# Process files in the given path\n",
    "for file_path in glob.glob(f\"{path}/*.json\"):\n",
    "    if \"_0\" in file_path:\n",
    "        continue\n",
    "\n",
    "    print(file_path.split(\"/\")[-1])\n",
    "    is_zh = \"deen\" not in file_path\n",
    "\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Extract source and target texts\n",
    "    sources = [_extract_text(item[\"prompt\"], is_zh=is_zh) for item in data]\n",
    "    translations = [item[\"predict\"] for item in data]\n",
    "\n",
    "    # Calculate metrics\n",
    "    lex_density = calculate_global_lexical_density(translations, is_zh=is_zh)\n",
    "    len_variety = [\n",
    "        calculate_length_variety(s, t, source_is_zh=False, target_is_zh=is_zh)\n",
    "        for s, t in tqdm(zip(sources, translations))\n",
    "    ]\n",
    "\n",
    "    # Store results\n",
    "    results[file_path.split(\"/\")[-1]] = {\n",
    "        \"lex_density\": lex_density,\n",
    "        \"len_variety\": np.mean(len_variety),\n",
    "    }\n",
    "\n",
    "    print(f\"Lexical Density: {lex_density}, Length Variety: {np.mean(len_variety)}\")\n",
    "\n",
    "# Output results\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914224f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
